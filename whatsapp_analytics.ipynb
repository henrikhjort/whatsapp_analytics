{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert exported WhatsApp chat into data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.preprocessing import normalize\n",
    "from matplotlib.pyplot import cm\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def filter_messages(messages):\n",
    "    # Remove all media messages.\n",
    "    w = [x for x in messages if not \"<media omitted>\" in x]\n",
    "    \n",
    "    # Remove all subject changed messages\n",
    "    w = [x for x in w if not 'changed the subject from \"' in x]\n",
    "    \n",
    "    # Remove all group icon changed messages.\n",
    "    w = [x for x in w if not \" changed this group's icon\" in x]\n",
    "    \n",
    "    # Remove all group description messages the group description\n",
    "    w = [x for x in w if not \"deleted the group description\" in x]\n",
    "    w = [x for x in w if not \"changed the group description\" in x]\n",
    "    \n",
    "    w = [x for x in w if not \" added\" in x]\n",
    "    w = [x for x in w if not \" left\" in x]\n",
    "    w = [x for x in w if not \" removed\" in x]\n",
    "\n",
    "    return w\n",
    "\n",
    "def map_senders_to_int(senders):\n",
    "    uniques = np.unique(senders)\n",
    "    senders_int = []\n",
    "    for sender in senders:\n",
    "        senders_int.append(np.where(uniques==sender)[0][0])\n",
    "    return np.asarray(senders_int)\n",
    "\n",
    "\n",
    "def create_data_matrix(data):\n",
    "    lines = data.split('\\n')\n",
    "    lines = list(filter(None, lines))\n",
    "    lines = filter_messages(lines)\n",
    "    \n",
    "    timestamps = []\n",
    "    senders = []\n",
    "    messages = []\n",
    "    \n",
    "    for index, line in enumerate(lines):\n",
    "        date = line[12:17] + '/' + line[:10]\n",
    "\n",
    "        try:\n",
    "            # try this? \n",
    "            # const match = line.match(/([^-]+)\\s-\\s([^:]+):\\s(.+)/);\n",
    "            # If the timestamp is valid we can assume that sender is found from the line\n",
    "            # line[19:] contains a string that is of type 'sender: lorem ipsum...'\n",
    "            # We find ':' and take all characters to its left\n",
    "            timestamp = datetime.datetime.strptime(date, '%H:%M/%d/%m/%Y')\n",
    "            sender = line[20:][:line[20:].find(':')].lower()\n",
    "            \n",
    "            message = line[19:][line[19:].find(':')+2:].lower()\n",
    "            \n",
    "            timestamps.append(timestamp)\n",
    "            senders.append(sender)\n",
    "            messages.append(message)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    senders_int = map_senders_to_int(senders)\n",
    "    \n",
    "    return np.column_stack((np.asarray(timestamps), np.asarray(senders), senders_int, np.asarray(messages)))\n",
    "\n",
    "\n",
    "def get_top_420(data_matrix):\n",
    "    blaze_it_am = datetime.datetime.strptime('04:20/01/01/1970', '%H:%M/%d/%m/%Y')\n",
    "    blaze_it_pm = datetime.datetime.strptime('16:20/01/01/1970', '%H:%M/%d/%m/%Y')\n",
    "    \n",
    "    # Indices of messages where time is 04:20 or 16:20\n",
    "    t_420 = []\n",
    "    \n",
    "    for i, t in enumerate(data_matrix.T[0]):\n",
    "        if t.time() == blaze_it_am.time() or t.time() == blaze_it_pm.time():\n",
    "            t_420.append(i)\n",
    "    t_420 = np.asarray(t_420)\n",
    "    \n",
    "    # Indices of messages that contain '420'\n",
    "    m_420 = []\n",
    "    \n",
    "    for i, msg in enumerate(data_matrix.T[3]):\n",
    "        if '420'in msg:\n",
    "            m_420.append(i)\n",
    "    \n",
    "    m_420 = np.asarray(m_420)\n",
    "    \n",
    "    # TODO: Filter multiple entries for same day (allow 04:20 and 16:20 separately)\n",
    "    \n",
    "    matches = data_matrix[np.intersect1d(t_420,m_420)].T[1]\n",
    "    values, counts = np.unique(matches,return_counts=True)\n",
    "    index = np.argmax(counts)\n",
    " \n",
    "    return values[index], counts[index]\n",
    "\n",
    "def get_list_of_senders(data_matrix):\n",
    "    return np.unique(data_matrix.T[1]), np.unique(data_matrix.T[2])\n",
    "\n",
    "\n",
    "def get_longest_rant(data_matrix):\n",
    "    senders_int = data_matrix.T[2]\n",
    "    #print(senders_int)\n",
    "    \n",
    "\n",
    "def get_activity_by_hour(data_matrix):\n",
    "    hours = []\n",
    "    for t in data_matrix.T[0]:\n",
    "        hours.append(t.hour)\n",
    "\n",
    "    hours = np.asarray(hours)\n",
    "\n",
    "    hour, count = np.unique(hours,return_counts=True)\n",
    "    \n",
    "    return hour,count\n",
    "\n",
    "def plot_activity_by_hour(x,y):\n",
    "    y_pos = np.arange(len(x))\n",
    "    plt.xticks(y_pos, x)\n",
    "    plt.plot(x,y)\n",
    "    plt.ylabel('Messages')\n",
    "    plt.grid()\n",
    "    plt.title('Activity by hour')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_activity_by_day(data_matrix):\n",
    "    # Monday is 0, Sunday is 6\n",
    "    weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    days = []\n",
    "    for t in data_matrix.T[0]:\n",
    "        days.append(t.weekday())\n",
    "\n",
    "    days = np.asarray(days)\n",
    "    \n",
    "    day, count = np.unique(days, return_counts=True)\n",
    "    \n",
    "    return weekdays, count\n",
    "\n",
    "\n",
    "def plot_activity_by_day(x,y):\n",
    "    y_pos = np.arange(len(x))\n",
    "    plt.xticks(y_pos, x)\n",
    "    plt.bar(y_pos, y, align='center', alpha=0.5)\n",
    "    plt.ylabel('Messages')\n",
    "    plt.grid()\n",
    "    plt.title('Activity by day')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_user_activity_by_day(user, data_matrix):\n",
    "    users, users_int = get_list_of_senders(data_matrix)\n",
    "    if not user in users:\n",
    "        raise Exception('User', user, 'not found from user list.')\n",
    "    messages_by_user = data_matrix[np.where(data_matrix.T[1]==user)]\n",
    "    weekdays, count = get_activity_by_day(messages_by_user)\n",
    "    \n",
    "    return weekdays, count\n",
    "\n",
    "def get_user_activity_by_hour(user, data_matrix):\n",
    "    hour_count = {0: 0, 1: 0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0 , 8:0, 9:0, 10:0, 11:0, 12:0, \n",
    "                      13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0, 21:0, 22:0, 23:0}\n",
    "    users, users_int = get_list_of_senders(data_matrix)\n",
    "    if not user in users:\n",
    "        raise Exception('User', user, 'not found from user list.')\n",
    "    messages_by_user = data_matrix[np.where(data_matrix.T[1]==user)]\n",
    "    dates = messages_by_user.T[0]\n",
    "    for date in dates:\n",
    "        h = date.hour\n",
    "        hour_count[h] = hour_count[h] + 1\n",
    "        \n",
    "    hours = np.arange(24)\n",
    "    count = np.asarray(list(hour_count.values()))\n",
    "    \n",
    "    return hours, count\n",
    "\n",
    "def plot_users_activity_by_day(data_matrix):\n",
    "    users, users_int = get_list_of_senders(data_matrix)\n",
    "    \n",
    "    usr = []\n",
    "    counts = []\n",
    "    \n",
    "    for user in users:\n",
    "        weekdays, count = get_user_activity_by_day(user, data_matrix)\n",
    "        if count.shape == np.zeros(7).shape:\n",
    "            counts.append(np.round(count / np.sum(count), decimals=2))\n",
    "            usr.append(users_int[users.index(user)])\n",
    "    counts = np.asarray(counts)\n",
    "    usr = np.asarray(usr)\n",
    "    \n",
    "    matrix = np.column_stack((usr,counts))\n",
    "    data = matrix.T[1:].astype(np.float)\n",
    "    \n",
    "    # Labels\n",
    "    x_labels = matrix.T[0]\n",
    "    y_labels = np.asarray(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(data, cmap=cm.Greens, interpolation='nearest')\n",
    "\n",
    "    ax.set_xticks(np.arange(len(x_labels)))\n",
    "    ax.set_yticks(np.arange(len(y_labels)))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    for i in range(len(y_labels)):\n",
    "        for j in range(len(x_labels)):\n",
    "            text = ax.text(j, i, data[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    ax.set_title('Distribution of messages across weekdays')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_users_activity_by_hour(data_matrix):\n",
    "    users, users_int = get_list_of_senders(data_matrix)\n",
    "    \n",
    "    usr = []\n",
    "    counts = []\n",
    "    \n",
    "    for user in users:\n",
    "        hours, count = get_user_activity_by_hour(user, data_matrix)\n",
    "        if count.shape == np.zeros(24).shape:\n",
    "            counts.append(count)\n",
    "            usr.append(user)\n",
    "    counts = np.asarray(counts)\n",
    "    usr = np.asarray(usr)\n",
    "    \n",
    "    matrix = np.column_stack((usr,counts))\n",
    "    data = matrix.T[1:].astype(np.float).T\n",
    "    \n",
    "    # Labels\n",
    "    x_labels = np.arange(24)\n",
    "    y_labels = matrix.T[0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(data, cmap=cm.Greens, interpolation='nearest')\n",
    "\n",
    "    ax.set_xticks(np.arange(len(x_labels)))\n",
    "    ax.set_yticks(np.arange(len(y_labels)))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    ax.set_title('Distribution of messages across hours')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_wordcloud(data_matrix):\n",
    "    text = data_matrix.T[3].flatten()\n",
    "    text = ''.join(c for c in text if c <= '\\uFFFF')\n",
    "    '''\n",
    "    stopwords = set(STOPWORDS)\n",
    "    sw = [\"media\", \"omitted\", \"ja\", \"se\", \"et\", \"mun\", \"ku\", \n",
    "                      \"ei\", \"sen\", \"sun\", \"jo\", \"vaa\", \"sit\", \"jos\", \"oli\", \n",
    "                      \"oo\", \"mut\", \"voi\", \"kyl\", \"joku\", \"en\", \"ne\", \"oon\", \"haluu\", \"miten\",\n",
    "                      \"oi\", \"ois\", \"oot\", \"sä\", \"nyt\", \"iha\", \"hyvä\", \"sitä\", \"tehä\",\n",
    "                      \"mitä\", \"toi\", \"nii\", \"tää\", \"mä\", \"aika\", \"tulee\", \"olla\", \"tehää\", \"vittu\",\n",
    "                      \"tai\", \"olla\", \"kaikki\", \"message\", \"saa\", \"tai\", \"ollu\", \"ees\", \"mis\", \"ton\",\n",
    "                       \"varmaa\", \"joka\", \"vähä\", \"ollu\", \"mikä\", \"varmaa\", \"pitää\", \"joo\",\n",
    "                       \"aina\", \"mik\", \"koska\", \"ni\", \"viel\", \"menee\", \"tänää\", \"siel\",\n",
    "                       \"kans\", \"jotai\", \"mi\", \"mulle\", \"että\", \"max\", \"juuso\", \"vitun\",\n",
    "                       \"siis\", \"miks\", \"vois\", \"ite\", \"taas\", \"tuli\", \"sano\", \"sanoo\"]\n",
    "    stopwords.update(sw)\n",
    "    '''\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(['media', 'omitted', 'omitted', 'media', 'et', 'ja', 'se', 'ei', 'ku', 'jo', 'mut'])\n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
    "    #wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    # the matplotlib way:\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_user_wordcloud(user):\n",
    "    messages_by_user = data_matrix[np.where(data_matrix.T[1]==user)]\n",
    "    plot_wordcloud(messages_by_user)\n",
    "    \n",
    "\n",
    "def get_n_most_common_words(n, data_matrix):\n",
    "    words = []\n",
    "    for message in data_matrix.T[3].flatten():\n",
    "        split = message.split()\n",
    "        for w in split:\n",
    "            words.append(w)\n",
    "    counter = Counter(words)\n",
    "    return counter.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_file('data.txt')\n",
    "\n",
    "# Columns: 0 timestamp, 1 sender, 2 sender as int, 3 message\n",
    "data_matrix = create_data_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics\n",
    "\n",
    "# Wordcloud\n",
    "plot_wordcloud(data_matrix)\n",
    "\n",
    "top_blazer, hits = get_top_420(data_matrix)\n",
    "print(top_blazer, hits)\n",
    "\n",
    "#get_longest_rant(data_matrix)\n",
    "\n",
    "# Hourly activity\n",
    "hour, count = get_activity_by_hour(data_matrix)\n",
    "plot_activity_by_hour(hour,count)\n",
    "\n",
    "# Daily activity\n",
    "day, count = get_activity_by_day(data_matrix)\n",
    "plot_activity_by_day(day, count)\n",
    "\n",
    "# User activity by weekday\n",
    "plot_users_activity_by_day(data_matrix)\n",
    "\n",
    "# User activity by hour\n",
    "plot_users_activity_by_hour(data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def get_tokens(data_matrix):\n",
    "    messages = data_matrix.T[3]\n",
    "    tokens = []\n",
    "    for msg in messages:\n",
    "        words_in_msg = msg.split(' ')\n",
    "        for word in words_in_msg:\n",
    "            tokens.append(word)\n",
    "    tokens = list(set(tokens))\n",
    "    \n",
    "    print(len(tokens), 'tokens')\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Create feature matrix\n",
    "# [hour, day, message vector]\n",
    "\n",
    "def create_feature_matrix(data_matrix):\n",
    "    date = data_matrix.T[0]\n",
    "    \n",
    "    helper = np.vectorize(lambda x: x.hour)\n",
    "    hour_vector = np.asarray(helper(date))\n",
    "    \n",
    "    helper = np.vectorize(lambda x: x.weekday())\n",
    "    day_vector = np.asarray(helper(date))\n",
    "\n",
    "    # create CountVectorizer object\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    corpus = data_matrix.T[3]\n",
    "\n",
    "    # learn the vocabulary and store CountVectorizer sparse matrix in X\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # columns of X correspond to the result of this method\n",
    "    tokens = get_tokens(data_matrix)\n",
    "    vectorizer.get_feature_names() == (tokens)\n",
    "\n",
    "    # retrieving the matrix in the numpy form\n",
    "    X.toarray()\n",
    "\n",
    "    # transforming a new document according to learn vocabulary\n",
    "    # vectorizer.transform(['hae lava']).toarray()\n",
    "    msg_vector = []\n",
    "    messages = data_matrix.T[3]\n",
    "    \n",
    "    for msg in messages:\n",
    "        msg = msg.split(' ')\n",
    "        t = vectorizer.transform(msg).toarray()\n",
    "        #print(t.shape)\n",
    "        msg_vector.append(t)\n",
    "        \n",
    "    msg_vector = np.asarray(msg_vector)\n",
    "    \n",
    "    feature_matrix = np.column_stack((hour_vector, day_vector, msg_vector))\n",
    "    print('Feature matrix shape:')\n",
    "    print(feature_matrix.shape)\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour, day, message\n",
    "X = create_feature_matrix(data_matrix)\n",
    "Y = data_matrix.T[2]\n",
    "\n",
    "print('Label vector shape')\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_tokens(data_matrix)\n",
    "\n",
    "def message_to_vector(message):\n",
    "    init = np.zeros(len(tokens))\n",
    "    for w in message:\n",
    "        print(w)\n",
    "        indices = np.where(tokens == w)\n",
    "        print(indices)\n",
    "    values = np.ones(indices[0].shape)\n",
    "    print(values.shape)\n",
    "    a = np.put(init, indices, values)\n",
    "    print(np.where(a==1))\n",
    "    \n",
    "message_to_vector(['isompi', 'maksoks'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
